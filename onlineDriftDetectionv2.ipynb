{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "onlineDriftDetectionv2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eliazino/.netPDO/blob/master/onlineDriftDetectionv2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75RLiZ_hzKMz"
      },
      "source": [
        "import pandas as pd\n",
        "import graphviz\n",
        "import math\n",
        "import numpy as np\n",
        "import random\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding\n",
        "from keras.layers import LSTM\n",
        "from matplotlib import pyplot as plt\n",
        "from keras.models import load_model\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from sklearn.model_selection import KFold\n",
        "from keras.layers import Dropout\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils.vis_utils import plot_model\n",
        "import pydotplus as pydot\n",
        "from graphviz import Digraph\n",
        "import copy\n",
        "import csv\n",
        "from google.colab import files\n",
        "from collections import namedtuple"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ES4EIOW39tTg"
      },
      "source": [
        "class dataDivider:\n",
        "  def __init__(self, data):\n",
        "    self.data = data\n",
        "    self.tracesPerPart = 0\n",
        "    self.totalCount = 0\n",
        "  def getPartDictionary(self, labels):\n",
        "    indexes = []\n",
        "    _activeCase = None\n",
        "    caseLabel = labels.case;\n",
        "    indexi = []\n",
        "    index = 0\n",
        "    for index, row in self.data.iterrows():\n",
        "      if (_activeCase == None):\n",
        "        _activeCase = row[caseLabel]\n",
        "        indexi = [index, -1]\n",
        "      else:\n",
        "        if (_activeCase != row[caseLabel]):\n",
        "          indexi[1] = index\n",
        "          indexes.append(tuple(indexi))\n",
        "          indexi[0] = index + 1\n",
        "          _activeCase = row[caseLabel]\n",
        "    indexi[1] = index + 1\n",
        "    indexes.append(tuple(indexi))\n",
        "    return indexes\n",
        "  def setParts(self, labels, parts):\n",
        "    indexes = self.getPartDictionary(labels)\n",
        "    if parts > len(indexes):\n",
        "      raise ValueError('Part cannot be greater than total events') \n",
        "    approxSize = round(len(indexes)/parts)\n",
        "    partIndexes = []\n",
        "    partCount = []\n",
        "    for i in range(0, parts):\n",
        "      top = i * approxSize\n",
        "      bottom = top + approxSize -1\n",
        "      startIndex = indexes[top][0]\n",
        "      endIndex = indexes[bottom][1];\n",
        "      partIndexes.append([startIndex, endIndex])\n",
        "      partCount.append(approxSize)\n",
        "    self.partIndexes = partIndexes\n",
        "    self.totalCount = len(indexes)\n",
        "    self.tracesPerPart = approxSize\n",
        "    return partIndexes\n",
        "  def getPartIndex(self, indx):\n",
        "    return self.data.iloc[self.partIndexes[indx][0]], self.data.iloc[self.partIndexes[indx][1]];\n",
        "  def getPart(self, index):\n",
        "    return self.data.iloc[self.partIndexes[index][0]:self.partIndexes[index][1], :]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FxfTRcBj2PHN"
      },
      "source": [
        "class prepareData:\n",
        "  def __init__(self, data, label):\n",
        "    self.data = data\n",
        "    self.label = label\n",
        "  def create_input_output(self, xy):\n",
        "    # Define Empty List\n",
        "    values = []\n",
        "    xList = []\n",
        "    _ncols = ('X', 'Y')\n",
        "    values.append((\"NULL\", xy[0]))\n",
        "    i = 0\n",
        "    while i < len(xy):\n",
        "        try:\n",
        "            xList = xy[0: i+1]\n",
        "            xList.insert(0, \"NULL\")\n",
        "            values.append((xList, xy[i + 1]))\n",
        "        except:\n",
        "            xList = xy[0: i+1]\n",
        "            xList.insert(0, \"NULL\")\n",
        "            values.append((xList, \"END\"))\n",
        "        i = i + 1\n",
        "    return pd.DataFrame(values, columns=_ncols) \n",
        "\n",
        "  def prepare(self, validEvts = None, test_size = 0, tokenizer = None):\n",
        "    nameLabel = self.label[0]\n",
        "    valueLabel = self.label[1]\n",
        "    _activeCase = \"NULL\"\n",
        "    _tempxy = []\n",
        "    _ncols = ('X', 'Y')\n",
        "    maindfObj = pd.DataFrame([], columns=_ncols)\n",
        "    if validEvts is not None:\n",
        "      helperObj = helper()\n",
        "      validEvts = helperObj.oneDimStrToLower(validEvts)\n",
        "    for index, row in self.data.iterrows():\n",
        "      if validEvts is not None and row[valueLabel].lower() not in validEvts:\n",
        "        continue\n",
        "      if nameLabel in row and (row[nameLabel] == _activeCase or _activeCase == \"NULL\"):\n",
        "        concatenatedString = row[valueLabel]\n",
        "        _tempxy.append(concatenatedString)\n",
        "        _activeCase = row[nameLabel]\n",
        "      else:\n",
        "        subObject = self.create_input_output(_tempxy)\n",
        "        maindfObj = maindfObj.append(subObject)\n",
        "        _activeCase = row[nameLabel]\n",
        "        _tempxy.clear()\n",
        "        concatenatedString = row[valueLabel]\n",
        "        _tempxy.append(concatenatedString)\n",
        "    self.tokenize(maindfObj, tokenizer)\n",
        "    self.maindfObj = maindfObj\n",
        "    return self.custom_split(self.X, self.Y, test_size)\n",
        "\n",
        "  def append_to_2d(self, former_2d, new_2d):\n",
        "    for i in range(len(new_2d)):\n",
        "      former_2d.append(new_2d[i])\n",
        "    return former_2d\n",
        "\n",
        "  def custom_split(self, X, Y, test_size):\n",
        "    Xtrain = []\n",
        "    Ytrain = []\n",
        "    Xtest = []\n",
        "    Ytest = []\n",
        "    size = X.shape  \n",
        "    import random\n",
        "    startList = []\n",
        "    endList = []\n",
        "    for i in range(size[0]):\n",
        "      consid = X[i]\n",
        "      if consid[len(consid) - 2] == 0:\n",
        "        startList.append(i)\n",
        "        if(i > 0):\n",
        "          endList.append(i-1)\n",
        "    endList.append(size[0]-1) #Tail End of the Array is the last element of endList\n",
        "    num_test = int(round(len(startList)*test_size))  \n",
        "    num_train = len(startList) - num_test    \n",
        "    t = random.sample(startList, num_test)\n",
        "    counter = 0\n",
        "    for i in startList:\n",
        "      Xcase = np.array(X[i:endList[counter]+1])\n",
        "      Ycase = np.array(Y[i:endList[counter]+1])\n",
        "      if (i in t):\n",
        "        Xtest = self.append_to_2d(Xtest, Xcase)\n",
        "        Ytest = self.append_to_2d(Ytest, Ycase)\n",
        "      else:\n",
        "        Xtrain = self.append_to_2d(Xtrain, Xcase)\n",
        "        Ytrain = self.append_to_2d(Ytrain, Ycase)\n",
        "      counter = counter + 1\n",
        "    return np.array(Xtrain), np.array(Xtest), np.array(Ytrain), np.array(Ytest)\n",
        "\n",
        "  def tokenize(self, data, tokenizer):\n",
        "    if tokenizer is None:\n",
        "      tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
        "      tokenizer.fit_on_texts(data['X'])\n",
        "    X = tokenizer.texts_to_sequences(data['X'])\n",
        "    word_index = tokenizer.word_index\n",
        "    print(word_index)\n",
        "    print('Found %s unique tokens.' % len(word_index))\n",
        "    X = pad_sequences(X)\n",
        "    Y = pd.get_dummies(data['Y'])\n",
        "    self.X = X\n",
        "    self.Y = Y\n",
        "    self.tokenizer = tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_LfY-qkVT1X"
      },
      "source": [
        "class helper:\n",
        "  def __init__(self):\n",
        "    self.i= 0\n",
        "  def datasetListMergeMinus(self, dataset, subset):\n",
        "    wholeData = 1\n",
        "    for m in dataset:\n",
        "      if not m.equals(subset):\n",
        "        if type(wholeData) is int:\n",
        "          wholeData = m          \n",
        "        else:\n",
        "          wholeData = wholeData.append(m)\n",
        "    return wholeData\n",
        "  def multiDimStrToUpper(self, string):\n",
        "    nstring = []\n",
        "    for strns in string:\n",
        "      nstring.append([x.upper() for x in strns])\n",
        "    return nstring\n",
        "  def multiDimStrToLower(self, string):\n",
        "    nstring = []\n",
        "    for strns in string:\n",
        "      nstring.append([x.lower() for x in strns])\n",
        "    return nstring\n",
        "  def oneDimStrToLower(self, string):\n",
        "    nstring = []\n",
        "    for i in range(0, len(string)):\n",
        "      nstring.append(string[i].lower())\n",
        "    return nstring\n",
        "  def grabEventsFromHeader(self, header):\n",
        "    evs = []\n",
        "    c = 0\n",
        "    for ev in header:\n",
        "      if c > 0:\n",
        "        try:\n",
        "          num = int(ev)\n",
        "        except:\n",
        "          evs.append(ev)\n",
        "      c = c + 1\n",
        "    return evs\n",
        "\n",
        "  def rowIsFirst(self, row, activities, headers):\n",
        "    foundValues = []\n",
        "    for i in range(0,len(headers)):\n",
        "      val = headers[i]\n",
        "      if row[val] in activities:\n",
        "        foundValues.append(row[val])\n",
        "    if (len(foundValues) == 0):\n",
        "      return True\n",
        "    return False\n",
        "\n",
        "  def rowIsLast(self, row, evName):\n",
        "    rowEv = row.idxmax()\n",
        "    try:\n",
        "      if rowEv.lower() == evName.lower():\n",
        "        return True\n",
        "    except:\n",
        "      return False\n",
        "    return False\n",
        "\n",
        "  def divideMatrix(self, matrix):\n",
        "    headers = list(matrix.columns.values)\n",
        "    leftHeaders = []\n",
        "    rightHeaders = []\n",
        "    for i in range(0,len(headers)):\n",
        "      ev = headers[i]\n",
        "      try:\n",
        "          num = int(ev)        \n",
        "          leftHeaders.append(ev)        \n",
        "      except:\n",
        "          rightHeaders.append(ev)\n",
        "    leftData = matrix[leftHeaders]\n",
        "    rightData = matrix[rightHeaders]\n",
        "    return [leftData, rightData]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTeO1DxWVbA_"
      },
      "source": [
        "class training:\n",
        "  def __init__(self, X, Y):\n",
        "    self.X = X\n",
        "    self.Y = Y\n",
        "    MAX_NB_WORDS =415   #50\n",
        "    EMBEDDING_DIM =415   #32\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\n",
        "    model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
        "    model.add(Dense(Y.shape[1], activation='softmax'))\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    self.model = model\n",
        "  def train(self, tindx = '0'):\n",
        "    model = self.model\n",
        "    model.summary()\n",
        "    plot_model(model, to_file='model.png', show_shapes=True)\n",
        "    print('Training...')\n",
        "    X = self.X\n",
        "    Y = self.Y\n",
        "    history = model.fit(X, Y,  epochs=50, batch_size=1500, verbose=0)\n",
        "    self.model = model\n",
        "    model.save('Orig_model_'+tindx+'.h5')\n",
        "    return model.evaluate(X, Y)\n",
        "\n",
        "  def getModelFrom(self, modelprefix):\n",
        "    try:\n",
        "      modelName = 'Orig_model_'+modelprefix+'.h5';\n",
        "      self.model = load_model(modelName)\n",
        "    except:\n",
        "      self.train(modelprefix)\n",
        "\n",
        "  def align(self, from_, to_):\n",
        "    originalColumnNamesArr = to_.columns.values\n",
        "    driftedColumnNamesArr = from_.columns.values\n",
        "    colNum = []\n",
        "    for i in range(0, len(driftedColumnNamesArr)):\n",
        "      col = driftedColumnNamesArr[i]\n",
        "      if(col not in originalColumnNamesArr):\n",
        "        from_ = from_.drop(col, 1)\n",
        "        colNum.append(i)\n",
        "    for i in range(0, len(originalColumnNamesArr)):\n",
        "      col = originalColumnNamesArr[i]\n",
        "      if(col not in driftedColumnNamesArr):\n",
        "        from_[col] = 0\n",
        "    return from_, colNum\n",
        "\n",
        "  def validateModel(self, Prep_data, tokenizer, model,  X, Y):\n",
        "    predict_proba = model.predict(X)\n",
        "    colName = []\n",
        "    for i in Y:\n",
        "        colName.append(i)\n",
        "    print(predict_proba.shape)\n",
        "    print(len(colName))\n",
        "    dfObj = pd.DataFrame(list(np.round(predict_proba*100, decimals=0)), columns = colName)\n",
        "    Seq_Series=Prep_data.X.apply(pd.Series)\n",
        "    dfObj.reset_index(drop=True, inplace=True)\n",
        "    Seq_Series.reset_index(drop=True, inplace=True)\n",
        "    df_new = pd.concat([Seq_Series, dfObj], axis=1)\n",
        "    return df_new"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_kWHt7DVjf2"
      },
      "source": [
        "class resultGraphing:\n",
        "  def __init__(self):\n",
        "    self.i= 0\n",
        "  def decomposeResult(self, results):\n",
        "    helper_ = helper()\n",
        "    headers = list(results.columns.values)\n",
        "    events = helper_.grabEventsFromHeader(headers)\n",
        "    matrices = []\n",
        "    matricesLeft = []\n",
        "    lastIndex = 0\n",
        "    totalBegin = 0\n",
        "    ind = 0\n",
        "    newMatrix = helper_.divideMatrix(results)\n",
        "    for index, row in newMatrix[1].iterrows():\n",
        "      ind = index\n",
        "      rowIsLast = helper_.rowIsLast(row, \"end\")\n",
        "      if rowIsLast:\n",
        "        sequenceList = newMatrix[1].iloc[lastIndex:index+1, :]\n",
        "        sequenceListLeft = newMatrix[0].iloc[lastIndex:index+1, :]\n",
        "        matrices.append(sequenceList)\n",
        "        matricesLeft.append(sequenceListLeft)\n",
        "        lastIndex = index + 1\n",
        "    return matrices, matricesLeft\n",
        "\n",
        "  def rowIsAnewSequence(self, row):\n",
        "    try:\n",
        "      return math.isnan(row[\"1\"])\n",
        "    except:\n",
        "      return False\n",
        "\n",
        "  def linkAndProbabilities(self, matrices, count = 0):\n",
        "    links = []\n",
        "    probabilities = []\n",
        "    sequences = []\n",
        "    uniqueEvs = []\n",
        "    for i in range(0,len(matrices)):\n",
        "      thisMatrix = matrices[i]\n",
        "      lastEvent = \"Start\"\n",
        "      sequence = []\n",
        "      for index, row in thisMatrix.iterrows():\n",
        "        row = pd.to_numeric(row)\n",
        "        #print(row) # prints the rows\n",
        "        evName = row.idxmax(axis=1) # picks event with the highest probability\n",
        "        link = lastEvent + \"<-->\" + evName\n",
        "        sequence.append(evName)\n",
        "        if link not in links:\n",
        "          if lastEvent != evName:\n",
        "            if not (lastEvent.lower() == \"start\" and evName.lower() == \"end\"):\n",
        "              links.append(link)\n",
        "              prob = row[evName]\n",
        "              probabilities.append(prob)\n",
        "        lastEvent = evName\n",
        "      # The Last element is End and undesirable\n",
        "      sequence.pop()\n",
        "      sequences.append(sequence)\n",
        "    return links, probabilities, sequences\n",
        "\n",
        "  def drawGraph(self, transitions, counter):\n",
        "    G = Digraph('process_model', filename='dum_'+str(counter)+'.gv')\n",
        "    G.attr(rankdir='LR', size='7,5')\n",
        "    G.attr('node', shape='doublecircle', style=\"filled\", fillcolor=\"grey\")\n",
        "    G.node('Start')\n",
        "    G.node('END')\n",
        "    G.attr('node', shape='box', style=\"bold\")\n",
        "    for i in range(0,len(transitions)):\n",
        "      G.attr('edge', style=\"bold\", penwidth='3.0')\n",
        "      fromto = transitions[i].split(\"<-->\")\n",
        "      G.edge(fromto[0], fromto[1])\n",
        "    G.view()\n",
        "    return G\n",
        "  \n",
        "  def getEventSequence(self, data, X_label, Y_label):\n",
        "    currentX_label = ''\n",
        "    sequences = []\n",
        "    sequence = []\n",
        "    for index, row in data.iterrows():\n",
        "      if currentX_label == row[X_label]:\n",
        "        sequence.append(row[Y_label])\n",
        "      else:\n",
        "        if len(sequence) > 1:\n",
        "          sequences.append(sequence)\n",
        "        sequence = []\n",
        "        sequence.append(row[Y_label])\n",
        "      currentX_label = row[X_label]\n",
        "    sequences.append(sequence)\n",
        "    return sequences  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3O-oDgrnWdOT"
      },
      "source": [
        "class performance:\n",
        "  def __init__(self):\n",
        "    self.i= 0\n",
        "    \n",
        "  def fitness(self, holdOut, sequences):\n",
        "    TruePositives = 0\n",
        "    Count = 0\n",
        "    searched = []\n",
        "    for i in range(0,len(holdOut)):\n",
        "      found = holdOut[i] in sequences      \n",
        "      alreadySearched = holdOut[i] in searched\n",
        "      searched.append(holdOut[i])\n",
        "      if alreadySearched:\n",
        "        Count = Count + 1\n",
        "      #else:\n",
        "        #count = count + 1\n",
        "      if found:\n",
        "        if alreadySearched:\n",
        "          TruePositives = TruePositives + 1\n",
        "        else:\n",
        "          TruePositives = TruePositives + 1\n",
        "\n",
        "    print(\"The fitness of the discovered model against the holdout part\")\n",
        "    print(\" No. of True Positive: \" , TruePositives)\n",
        "    print(\" No. of Traces in holdout: \", len(holdOut))\n",
        "    return (TruePositives/len(holdOut))\n",
        "\n",
        "  def precision(self, original, sequences):\n",
        "    TruePositives = 0\n",
        "    Count = 0\n",
        "    searched = []\n",
        "    for i in range(0,len(original)):\n",
        "      found = original[i] in sequences\n",
        "      alreadySearched = original[i] in searched\n",
        "      searched.append(original[i])\n",
        "      if alreadySearched:\n",
        "        Count = Count + 1\n",
        "      if found:\n",
        "        if alreadySearched:\n",
        "          TruePositives = TruePositives + 1\n",
        "        else:\n",
        "          TruePositives = TruePositives + 1\n",
        "\n",
        "    print(\"The precision of the discovered model against the complete log\")\n",
        "    print(\" No. of True Positive: \", TruePositives)\n",
        "    print(\"No. of Traces in the model: \", len(sequences))\n",
        "    return (TruePositives/len(original))\n",
        "\n",
        "  def findFScore(self, fitness, precision):\n",
        "    a = fitness\n",
        "    b = precision\n",
        "    return (2 * (a * b)/(a + b))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfj-84QQ8AWW"
      },
      "source": [
        "class DatasetDefinitions:\n",
        "  def getUnique(self, label, dataset):\n",
        "    chains = []\n",
        "    chainTag = []\n",
        "    lastActivity = None\n",
        "    lastCase = None\n",
        "    eventLabel = label.event\n",
        "    caseLabel = label.case\n",
        "    for index, row in dataset.iterrows():\n",
        "      if lastCase is None or lastCase != row[caseLabel]:\n",
        "        lastActivity = None\n",
        "      if lastActivity is None:\n",
        "        lastActivity = row[eventLabel]\n",
        "        lastCase = row[caseLabel]\n",
        "        continue\n",
        "      lastCase = row[caseLabel]\n",
        "      if lastActivity != row[eventLabel]:\n",
        "        evChain = lastActivity +\"\"+row[eventLabel]\n",
        "        evChain = evChain.lower().strip()\n",
        "        evChain = \" \".join(evChain.split()).replace(' ', '_')\n",
        "        if evChain not in chains:\n",
        "          chains.append(evChain)\n",
        "          chainTag.append(lastCase)\n",
        "      lastActivity = row[eventLabel]\n",
        "    return chains, chainTag"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eIvcgnWU9g5P"
      },
      "source": [
        "class FindDrift:\n",
        "  def __init__(self, baseModel):\n",
        "    self.baseModel = baseModel\n",
        "  def executeAgainst(self, dataset, label):\n",
        "    _datasetDefinitions = DatasetDefinitions()\n",
        "    chain_base, tag_base = _datasetDefinitions.getUnique(label, self.baseModel)\n",
        "    chain, tag = _datasetDefinitions.getUnique(label, dataset)\n",
        "    indx = self.getDrifts(chain_base, chain)\n",
        "    tag_list = [tag[i] for i in indx]\n",
        "    chain_list = [chain[i] for i in indx]\n",
        "    return tag_list, chain_list, indx\n",
        "  def getDrifts(self, chain_base, chain_drift):\n",
        "    indx = []\n",
        "    counter = 0\n",
        "    for c in chain_drift:\n",
        "      if(c not in chain_base):\n",
        "        indx.append(counter)\n",
        "      counter = counter + 1\n",
        "    return indx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jd6SRiRQWexE"
      },
      "source": [
        "labels = namedtuple(\"labels\", \"case event\")\n",
        "label = labels('case', 'event')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WgRoSu7tWssb"
      },
      "source": [
        "parts = 10\n",
        "dataset = pd.read_csv('re-2500.csv', low_memory= False)\n",
        "data_divider = dataDivider(dataset)\n",
        "p_ = data_divider.setParts(label, parts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJ9dQHB8o2em",
        "outputId": "4c807eb4-ddd0-4932-a76a-aa0862022b4f"
      },
      "source": [
        "print(data_divider.tracesPerPart)\n",
        "print(data_divider.totalCount)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "250\n",
            "2500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gnMVOk4WQ8AO"
      },
      "source": [
        "dataset = pd.read_csv('base_log.csv', low_memory= False)\n",
        "_findDrift = FindDrift(dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_HZ0FmTWBGsB"
      },
      "source": [
        "fScoreLog = []\n",
        "DriftLog = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "sm9qNPQaW3SV",
        "outputId": "b66ea41b-2429-4141-f3f8-09bce82e6ece"
      },
      "source": [
        "for i in range(0, parts):\n",
        "  j = i + 1\n",
        "  if(i == parts - 1):\n",
        "    j = 0 \n",
        "  #Get the log part to be used\n",
        "  referenceLog = data_divider.getPart(i)\n",
        "  detectionLog = data_divider.getPart(j)\n",
        "\n",
        "  #Prepare the data Reference\n",
        "  prepdata = prepareData(referenceLog, ['case', 'event'])\n",
        "  X_train, X_test, Y_train, Y_test = prepdata.prepare(None, 0)\n",
        "\n",
        "  #get built tokenizer and word_index\n",
        "  tokenizer = prepdata.tokenizer\n",
        "  X = prepdata.X\n",
        "  Y = prepdata.Y\n",
        "  v = tokenizer.word_index.keys()\n",
        "\n",
        "  #Prepare the data Detection\n",
        "  prepdata_t = prepareData(detectionLog, ['case', 'event'])\n",
        "  X_train_t, X_test_t, Y_train_t, Y_test_t = prepdata_t.prepare(list(v), 0, tokenizer) #list(v)  \n",
        "  tokenizer_t = prepdata_t.tokenizer\n",
        "  X_t = prepdata_t.X\n",
        "  Y_t = prepdata_t.Y\n",
        "\n",
        "  #Train the Reference Model\n",
        "  trainModel = training(X_train, Y_train)\n",
        "  trainModel.getModelFrom(str(i))\n",
        "\n",
        "  #Use on itself\n",
        "  resultDataset_o = trainModel.validateModel(prepdata.maindfObj, tokenizer, trainModel.model,  X, Y)\n",
        "\n",
        "  #Use on Detection log\n",
        "  resultDataset_t = trainModel.validateModel(prepdata_t.maindfObj, tokenizer_t, trainModel.model,  X_t, Y)\n",
        "  resultGraphing_ = resultGraphing()\n",
        "\n",
        "  #Reference Log\n",
        "  probs_o, seqs_o = resultGraphing_.decomposeResult(resultDataset_o)\n",
        "  link_o, probabilities_o, sequences_o = resultGraphing_.linkAndProbabilities(probs_o)\n",
        "\n",
        "  #Reference Log\n",
        "  probs_t, seqs_t = resultGraphing_.decomposeResult(resultDataset_t)\n",
        "  #Detection Log\n",
        "  link_t, probabilities_t, sequences_t = resultGraphing_.linkAndProbabilities(probs_t)\n",
        "\n",
        "  #Calculate performance\n",
        "  performance_ = performance()\n",
        "  fitness = performance_.fitness(sequences_o, sequences_t)\n",
        "  precision = performance_.precision(sequences_o, sequences_t)\n",
        "\n",
        "  fScore = performance_.findFScore(fitness, precision);\n",
        "  if fScore < 0.9:\n",
        "    g, c, drftIndx = _findDrift.executeAgainst(detectionLog, label)\n",
        "    if(len(g) > 0):\n",
        "      counter = 0\n",
        "      for u in g:\n",
        "        strt, ed = data_divider.getPartIndex(j);\n",
        "        logIndx = (data_divider.tracesPerPart * j) + (drftIndx[counter] + 1)\n",
        "        nu = \"Drift detected in \"+u+\" Log number \"+str(logIndx)+\" being \"\n",
        "        if(nu not in DriftLog):\n",
        "          DriftLog.append(nu)\n",
        "        counter = counter + 1\n",
        "  ra = (i, j, fScore)\n",
        "  fScoreLog.append(ra)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'check_application_form_completeness': 1, 'null': 2, 'loan_application_received': 3, 'check_credit_history': 4, 'appraise_property': 5, 'asses_loan_risk': 6, 'return_application_back_to_applicant': 7, 'receive_updated_application': 8, 'asses_elegibility': 9, 'prepare_acceptance_pack': 10, 'check_if_home_insurance_quote_is_requested': 11, 'verify_repayment_agreement': 12, 'reject_application': 13, 'send_home_insurance_quote': 14, 'send_acceptance_pack': 15, 'loan_application_rejected': 16, 'finish_process': 17, 'approve_application': 18, 'loan_application_approved': 19, 'cancel_application': 20, 'loan_application_canceled': 21}\n",
            "Found 21 unique tokens.\n",
            "{'check_application_form_completeness': 1, 'null': 2, 'loan_application_received': 3, 'check_credit_history': 4, 'appraise_property': 5, 'asses_loan_risk': 6, 'return_application_back_to_applicant': 7, 'receive_updated_application': 8, 'asses_elegibility': 9, 'prepare_acceptance_pack': 10, 'check_if_home_insurance_quote_is_requested': 11, 'verify_repayment_agreement': 12, 'reject_application': 13, 'send_home_insurance_quote': 14, 'send_acceptance_pack': 15, 'loan_application_rejected': 16, 'finish_process': 17, 'approve_application': 18, 'loan_application_approved': 19, 'cancel_application': 20, 'loan_application_canceled': 21}\n",
            "Found 21 unique tokens.\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 23, 415)           172225    \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 100)               206400    \n",
            "                                                                 \n",
            " dense (Dense)               (None, 21)                2121      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 380,746\n",
            "Trainable params: 380,746\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Training...\n",
            "106/106 [==============================] - 2s 16ms/step - loss: 0.2415 - accuracy: 0.8716\n",
            "(3364, 21)\n",
            "21\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-ef5c1669ace9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m   \u001b[0;31m#Use on Detection log\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m   \u001b[0mresultDataset_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidateModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepdata_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaindfObj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mX_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m   \u001b[0mresultGraphing_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresultGraphing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-49e07fa44315>\u001b[0m in \u001b[0;36mvalidateModel\u001b[0;34m(self, Prep_data, tokenizer, model, X, Y)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mvalidateModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPrep_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0mpredict_proba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m     \u001b[0mcolName\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1127\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1128\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1129\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1130\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1621, in predict_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1611, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1604, in run_step  **\n        outputs = model.predict_step(data)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1572, in predict_step\n        return self(x, training=False)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/input_spec.py\", line 263, in assert_input_compatibility\n        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" is '\n\n    ValueError: Input 0 of layer \"sequential\" is incompatible with the layer: expected shape=(None, 23), found shape=(None, 22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3cAxZxE3YN7n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b13d2e5-3caf-41bf-d337-4bb45ef45e55"
      },
      "source": [
        "  DriftLog"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Drift detected in case[250] Log number 502 being ',\n",
              " 'Drift detected in case[250] Log number 503 being ',\n",
              " 'Drift detected in case[250] Log number 504 being ',\n",
              " 'Drift detected in case[250] Log number 505 being ',\n",
              " 'Drift detected in case[250] Log number 511 being ',\n",
              " 'Drift detected in case[251] Log number 515 being ',\n",
              " 'Drift detected in case[251] Log number 516 being ',\n",
              " 'Drift detected in case[251] Log number 518 being ',\n",
              " 'Drift detected in case[252] Log number 521 being ',\n",
              " 'Drift detected in case[500] Log number 1002 being ',\n",
              " 'Drift detected in case[500] Log number 1003 being ',\n",
              " 'Drift detected in case[500] Log number 1004 being ',\n",
              " 'Drift detected in case[500] Log number 1005 being ',\n",
              " 'Drift detected in case[500] Log number 1011 being ',\n",
              " 'Drift detected in case[501] Log number 1015 being ',\n",
              " 'Drift detected in case[501] Log number 1020 being ',\n",
              " 'Drift detected in case[502] Log number 1021 being ',\n",
              " 'Drift detected in case[502] Log number 1023 being ',\n",
              " 'Drift detected in case[750] Log number 1505 being ',\n",
              " 'Drift detected in case[750] Log number 1506 being ',\n",
              " 'Drift detected in case[750] Log number 1507 being ',\n",
              " 'Drift detected in case[750] Log number 1508 being ',\n",
              " 'Drift detected in case[750] Log number 1510 being ',\n",
              " 'Drift detected in case[752] Log number 1512 being ',\n",
              " 'Drift detected in case[752] Log number 1518 being ',\n",
              " 'Drift detected in case[753] Log number 1521 being ',\n",
              " 'Drift detected in case[756] Log number 1526 being ',\n",
              " 'Drift detected in case[1000] Log number 2002 being ',\n",
              " 'Drift detected in case[1000] Log number 2003 being ',\n",
              " 'Drift detected in case[1000] Log number 2004 being ',\n",
              " 'Drift detected in case[1000] Log number 2005 being ',\n",
              " 'Drift detected in case[1000] Log number 2007 being ',\n",
              " 'Drift detected in case[1002] Log number 2011 being ',\n",
              " 'Drift detected in case[1003] Log number 2012 being ',\n",
              " 'Drift detected in case[1003] Log number 2018 being ',\n",
              " 'Drift detected in case[1006] Log number 2026 being ',\n",
              " 'Drift detected in case[0] Log number 7 being ',\n",
              " 'Drift detected in case[0] Log number 8 being ',\n",
              " 'Drift detected in case[0] Log number 9 being ',\n",
              " 'Drift detected in case[0] Log number 15 being ',\n",
              " 'Drift detected in case[1] Log number 17 being ',\n",
              " 'Drift detected in case[1] Log number 18 being ',\n",
              " 'Drift detected in case[1] Log number 19 being ',\n",
              " 'Drift detected in case[1] Log number 21 being ',\n",
              " 'Drift detected in case[6] Log number 26 being ']"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    }
  ]
}